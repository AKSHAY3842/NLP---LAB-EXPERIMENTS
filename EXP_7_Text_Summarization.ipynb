{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7qm2rRRGkJG9Qowz+XWiS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AKSHAY3842/NLP---LAB-EXPERIMENTS/blob/main/EXP_7_Text_Summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqpZEZY1NP0F",
        "outputId": "fadacfad-3654-4041-a331-ca0e4268538c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [token.lower() for token in tokens]\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "    preprocessed_text = ' '.join(filtered_tokens)\n",
        "    return preprocessed_text\n",
        "\n",
        "def generate_summary(text, num_sentences=2):\n",
        "    preprocessed_text = preprocess_text(text)\n",
        "\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform([preprocessed_text, text])\n",
        "\n",
        "    similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "    ranked_sentences = sorted(range(len(similarity_matrix[0])), key=lambda x: similarity_matrix[0][x], reverse=True)\n",
        "    summary = ' '.join([nltk.sent_tokenize(text)[i] for i in ranked_sentences[:num_sentences]])\n",
        "    return summary\n",
        "\n",
        "\n",
        "user_text = input(\"Enter the text to be summarized:\\n\")\n",
        "\n",
        "summary = generate_summary(user_text)\n",
        "print(\"Summary:\")\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OG3OWWtPNYSh",
        "outputId": "68109fe1-52b6-4dba-e174-06e3e8dbb289"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the text to be summarized:\n",
            "Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence  concerned with the interactions between computers and human language, in particular how to program computers  to process and analyze large amounts of natural language data. Challenges in natural language processing  frequently involve speech recognition, natural language understanding, and natural language generation.  Tokenization is the process of breaking text into words, phrases, symbols, or other meaningful elements called tokens.  The list of tokens becomes input for further processing, such as parsing or text mining. Tokenization is useful both  in linguistics (where it is a form of text segmentation) and in computer science, where it forms part of lexical analysis.  Summary: Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence  concerned with the interactions between computers and human language, in particular how to program computers  to process and analyze large amounts of natural language data. Challenges in natural language processing  frequently involve speech recognition, natural language understanding, and natural language generation.\n",
            "Summary:\n",
            "Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence  concerned with the interactions between computers and human language, in particular how to program computers  to process and analyze large amounts of natural language data. Challenges in natural language processing  frequently involve speech recognition, natural language understanding, and natural language generation.\n"
          ]
        }
      ]
    }
  ]
}